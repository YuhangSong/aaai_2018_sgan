\def\year{2017}\relax
%File: formatting-instruction.tex
\documentclass[letterpaper]{article} %DO NOT CHANGE THIS
\usepackage{aaai17}  %Required
\usepackage{times}  %Required
\usepackage{helvet}  %Required
\usepackage{courier}  %Required
\usepackage{url}  %Required
\usepackage{graphicx}  %Required
\usepackage{amsfonts}
\usepackage{amsmath}
\frenchspacing  %Required
\setlength{\pdfpagewidth}{8.5in}  %Required
\setlength{\pdfpageheight}{11in}  %Required
%PDF Info Is Required:
  \pdfinfo{
/Title (2017 Formatting Instructions for Authors Using LaTeX)
/Author (AAAI Press Staff)}
\setcounter{secnumdepth}{0}
 \begin{document}
% The file aaai.sty is the style file for AAAI Press
% proceedings, working notes, and technical reports.
%
\title{2017 Formatting Instructions \\for Authors Using \LaTeX{}}
\author{AAAI Press\\
Association for the Advancement of Artificial Intelligence\\
2275 East Bayshore Road, Suite 160\\
Palo Alto, California 94303\\
}
\maketitle
\begin{abstract}
\end{abstract}

\section{The Proposed $S$ Distance}
We consider the one dimensional case as a start, where $x_{r}$ are real samples sampled from distribution $\mathbb{P}_{r}$, and $x_{g}$ are generated samples sampled from distribution $\mathbb{P}_{g}$,
\begin{equation}\label{x-r}
   x_{r} \sim \mathbb{P}_{r}
\end{equation}
\begin{equation}\label{x-g}
  x_{g} \sim \mathbb{P}_{g}
\end{equation}
Note that both $x_{r}$ and $x_{g}$ are restricted between $[0,1]$. Following is the proposed $S$ distance,
\begin{equation}\label{s-distance}
  S(\mathbb{P}_r,\mathbb{P}_g)=\mathbb{E}_{x_g \sim \mathbb{P}_g} \{ | \int_{x_g}^{1} \mathbb{P}_r(x) d x - \int_{x_g}^{1} \mathbb{P}_g(x) d x | \}
\end{equation}
while the Wasserstein distance is defined to be,
\begin{equation}\label{w-distance}
  W(\mathbb{P}_r,\mathbb{P}_g)=\sup_{\|f\|_L \leq 1} \{ \mathbb{E}_{x_r \sim \mathbb{P}_r} [f(x_r)] - \mathbb{E}_{x_g \sim \mathbb{P}_g} [f(x_g)] \}
\end{equation}
Apparently, both $S$ and $W$ distance will be minimized if the $\mathbb{P}_r$ and $\mathbb{P}_g$ are identical.
To take a deeper insight of the advantage of the proposed $S$ distance, we consider the representation of these two distance at one specific sample $x_g$.
This is crucial, since when updating \textit{Generator} $G$, it only observe at a specific $x_g$, instead of having a whole sight of the distributions $\mathbb{P}_r$ and $\mathbb{P}_g$.
The $S$ at $x_g$ is,
\begin{equation}\label{s-distance-at-xg}
  S_{\mathbb{P}_r,\mathbb{P}_g}(x_g)= | \int_{x_g}^{1} \mathbb{P}_r(x) d x - \int_{x_g}^{1} \mathbb{P}_g(x) d x |
\end{equation}
while the $W$ at $x_g$ is,
\begin{equation}\label{w-distance-xg}
  W_{\mathbb{P}_r,\mathbb{P}_g}(x_g) = f(x) \approx \mathbb{P}_r(x) - \mathbb{P}_g(x)
\end{equation}
We can see that $S_{\mathbb{P}_r,\mathbb{P}_g}(x_g)$ consider how unbalance are the two distributions in a whole sight, while the $W_{\mathbb{P}_r,\mathbb{P}_g}(x_g)$ considers the unbalance of the two probabilities at this specific $x_g$. One can easily think of a $x_g$, where $W_{\mathbb{P}_r,\mathbb{P}_g}(x_g)$ is zero, but the distributions $\mathbb{P}_r$ and $\mathbb{P}_r$ are not identical, which means $W_{\mathbb{P}_r,\mathbb{P}_g}(x_g)$ is failing. But under this situation, $S_{\mathbb{P}_r,\mathbb{P}_g}(x_g)$ still gives a right direction for $x_g$ to update, since it observes the unbalance of $\mathbb{P}_r$ and $\mathbb{P}_r$ on each side of the $x_g$.

\section{GAN Based on $S$ Distance}
Following we proposed the method to achieve this $S$ distance in a GAN.
We still describe things in one-dimensional case to make it simple and straight forward.
For every $x_{r}, x_{g}$ pair, we sample $x_{\tau}$ between $x_{r}$ and $x_g$,
\begin{equation}\label{x-epsilon}
  x_{\tau} = \tau x_{r} + (1-\tau) x_g
\end{equation}
where
\begin{equation}\label{epsilon}
  \tau \sim U[0,1]
\end{equation}
Following, we consider our problem on a discrete space with interval of $\varepsilon\rightarrow 0$, we give every notation of $x$ a check mark, i.e., $\check{x}$, to mark that they are discrete value under interval $\varepsilon$.
Later on, we will derive limitation on $\varepsilon\rightarrow 0$, so that we can have a general conclusion on the continuous space.
Now consider a event denoted by: $\check{x}_\tau\overset{t}{=}\check{x}_n$, which means,
\begin{itemize}
  \item Sample $\check{x}_\tau$ for $t$ times, $\check{x}_n$ got sampled as $\check{x}_\tau$ at least for one time.
\end{itemize}
To be clear, $\check{x}_\tau$ $\check{x}_r$, $\check{x}_g$ are all random variables, and $\check{x}_n$ is a specific point.
Apparently, we have,
\begin{equation}\label{p-epsilon-pr-pg}
    P(\check{x}_\tau\overset{1}{=}\check{x}_n|\check{x}_r,\check{x}_g)=
    \begin{cases}
        \frac{1}{d/\varepsilon} &\mbox{$\check{x}_r<\check{x}_n<\check{x}_g,\check{x}_g<\check{x}_n<\check{x}_r$}\\
        0 &\mbox{else}
    \end{cases}
\end{equation}
where
\begin{equation}\label{d}
  d=|\check{x}_r-\check{x}_g|
\end{equation}
If we sample $\check{x}_\tau$ for $t$ times, where
\begin{equation}\label{T}
  t = d/{\delta}
\end{equation}
Then, we have,
\begin{eqnarray}\label{p-tau}
    && P(\check{x}_\tau\overset{t}{=}\check{x}_n|\check{x}_r,\check{x}_g) \nonumber\\
    &=& 1 - (1-P(\check{x}_\tau\overset{1}{=}\check{x}_n|\check{x}_r,\check{x}_g))^t \nonumber\\
    &=&
    \begin{cases}
        1 - (1-\frac{1}{d/\varepsilon})^{d/\delta} &\mbox{$\check{x}_r<\check{x}_n<\check{x}_g,\check{x}_g<\check{x}_n<\check{x}_r$}\\
        0 &\mbox{else}
    \end{cases}
\end{eqnarray}
Apparently, $\delta$ has to satisfy,
\begin{equation}\label{deltal-satisfy}
  \delta = z \varepsilon
\end{equation}
where $z \in Z^{+}$. And this $z$ is a fixed value as a hyper-parameter.
Now, we consider following limit,
\begin{eqnarray}\label{limit-constant}
    && \lim_{\delta=z\varepsilon,\varepsilon\rightarrow 0} (1-\frac{1}{d/\varepsilon})^{d/\delta} \nonumber\\
    \nonumber &=& \lim_{\delta=z\varepsilon,\varepsilon\rightarrow 0} e^{d/\delta \ln(1-\frac{1}{d/\varepsilon})} \\
    \nonumber &=& \lim_{\delta=z\varepsilon,\varepsilon\rightarrow 0} e^{\frac{\ln(\frac{d-\varepsilon}{d})}{\delta/d}} \\
    \nonumber &=& \lim_{\varepsilon\rightarrow0} e^{\frac{\frac{d}{d-\varepsilon}\frac{-1}{d}}{z/d}} \\
    \nonumber &=& e^{-1/z} \\
\end{eqnarray}
Put the conclusion of \eqref{limit-constant} into \eqref{p-tau}, we have,
\begin{eqnarray}\label{final-p-inter}
    && P(x_\tau=x_n|x_r,x_g) = \lim_{\varepsilon,\delta\rightarrow0} P(\check{x}_\tau\overset{t}{=}\check{x}_n|\check{x}_r,\check{x}_g) \nonumber\\
    &=&
    \begin{cases}
        1 - e^{-1/z} &\mbox{$x_r<x_n<x_g,x_g<x_n<x_r$}\\
        0 &\mbox{else}
    \end{cases}
\end{eqnarray}
where we have switch back to the continuous space and have this general conclusion.
Now, we propose our update rules for the \textit{Discriminator} $D$ with parameter $\theta$ to be optimized,
\begin{equation}\label{g-loss}
  \theta \longrightarrow \theta + \nabla_{\theta} \{ - |\nabla_{x_{\tau}}D^{\theta}(x_{\tau})-\frac{x_{r}-x_{g}}{|x_{r}-x_{g}|}|^2 \}
\end{equation}
which means we try to make $\nabla_{x_{\tau}}D^{\theta}(x_{\tau})$ approach $\frac{x_{r}-x_{g}}{|x_{r}-x_{g}|}$. Lets take a look at $\nabla_{x_{\tau}}D^{\theta}(x_{\tau})$ at a specific point $x_n$,
\begin{eqnarray}\label{d-at-xn}
    && \nabla_{x_{\tau}=x_n} D^{\theta}(x_{\tau}=x_n) \nonumber\\
    &=& \mathbb{E}_{\tau \sim U[0,1], x_r\sim\mathbb{P}_r,x_g\sim\mathbb{P}_g} \{\frac{x_{r}-x_{g}}{|x_{r}-x_{g}|}\} \nonumber\\
    &=& P(x_\tau=x_n|x_g<x_n<x_r) P(x_g<x_n<x_r) \nonumber\\
    && - P(x_\tau=x_n|x_r<x_n<x_g) P(x_r<x_n<x_g) \nonumber\\
\end{eqnarray}
which means the value of $\nabla_{x_{\tau}=x_n} D^{\theta}(x_{\tau}=x_n)$ is determined by the probability of it gets positive update and negative update.
Since \eqref{final-p-inter}, we know that
\begin{equation}\label{p-inter-conditional-1}
  P(x_\tau=x_n|x_g<x_n<x_r)=1 - e^{-1/z}
\end{equation}
\begin{equation}\label{p-inter-conditional-2}
  P(x_\tau=x_n|x_r<x_n<x_g)=1 - e^{-1/z}
\end{equation}
Put \eqref{p-inter-conditional-1} \eqref{p-inter-conditional-2} into \eqref{d-at-xn}, we have,
\begin{eqnarray}\label{explain-d-gradient}
    && \nabla_{x_{\tau}=x_n} D^{\theta}(x_{\tau}=x_n) \nonumber\\
    &=& [P(x_g<x_n<x_r) - P(x_r<x_n<x_g)](1 - e^{-1/z}) \nonumber\\
    &=& [P(x_g<x_n)P(x_n<x_r) \nonumber\\
    && - P(x_r<x_n)P(x_n<x_g)](1 - e^{-1/z}) \nonumber\\
    &=& [\int_{0}^{x_n}\mathbb{P}_g(x)dx \int_{x_n}^{1}\mathbb{P}_r(x)dx \nonumber\\
    && - \int_{0}^{x_n}\mathbb{P}_r(x)dx \int_{x_n}^{1}\mathbb{P}_g(x)dx](1 - e^{-1/z}) \nonumber\\
    &=& [\int_{x_n}^{1}\mathbb{P}_r(x)dx-\int_{x_n}^{1}\mathbb{P}_g(x)dx](1 - e^{-1/z})
\end{eqnarray}
Now, we can give the update rule of \textit{Generator} $G$ with parameter $\beta$ to be learnt, and we put \eqref{explain-d-gradient} in this update rule to have a clearer view on what $G$ is doing,
\begin{eqnarray}\label{g-loss}
  && \beta \nonumber\\
  & \longrightarrow & \beta + \nabla_{\beta} \{ D^{\theta}(G^{\beta}(x_g)) \} \nonumber\\
  & \longrightarrow & \beta + \nabla_{\beta} \{ \frac{\partial D^{\theta}(G^{\beta}(x_g))}{\partial G^{\beta}(x_g)} \frac{\partial G^{\beta}(x_g)}{\partial \beta} \} \nonumber\\
  & \longrightarrow & \beta + \{[\int_{x_g}^{1}\mathbb{P}_r(x)dx\nonumber\\
  && -\int_{x_g}^{1}\mathbb{P}_g(x)dx](1 - e^{-1/z})(\nabla_{\beta}G^{\beta}(x_g)) \} \nonumber\\
\end{eqnarray}
which means wherever $x_g$ is, it is updating itself to make $\int_{x_g}^{1}\mathbb{P}_g(x)dx$ approaching $\int_{x_g}^{1}\mathbb{P}_r(x)dx$.
One can just take a few cases to confirm this.
The absolute error when updating $G$ is actually modelling the proposed $S$ distance in \eqref{s-distance} \eqref{s-distance-at-xg}.

\end{document}
